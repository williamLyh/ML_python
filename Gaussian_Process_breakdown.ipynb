{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process\n",
    "reference: http://www.gaussianprocess.org/gpml/chapters/RW.pdf\n",
    "\n",
    "We have been inferring parameters of function $p(\\theta|D)$ rather than the function itself $p(f|D)$. For given input $x_i$ and output $y_i$, we assume there is $y_i = f(x_i )$. The optimal approach is to infer the $p(f|X,y)$ and use this to make predicitons:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(y_*| x_*,X,y) = \\int p(y_*|f,x_*)p(f|X,y) df\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Gaussian Process provides an approach to do this. GP defines a prior over functions, which can be converted into a posterior over functions once we have seen some data. Although it's hard to represent a distribution over a function, we can define a distribution over the function's values at a finite, but arbitrary set of points.\n",
    "\n",
    "GP gives well-calibrated probabilistic outputs, while some kernel methods like LIVM, RVM and SVM, do not.\n",
    "\n",
    "- ## Regression\n",
    "Let the prior on the regression function be a GP:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(x) \\sim GP(m(x),\\kappa(x,x'))\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m(x) &= E[f(x)] \\\\\n",
    "\\kappa(x,x') &= E[(f(x)-m(x))(f(x)-m(x'))^T] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For any finite set of points:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(\\mathbf{f|X}) = N(\\mathbf{f|\\mu,K})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The mean function is usually set to be 0, as the GP is flexible enough to model the mean arbitrarily well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions using noise-free observation\n",
    "\n",
    "When we assume the observations are noiseless, we expect, for train data $\\mathbf{x}$, $f(\\mathbf{x})$ has no uncertainty. By definition of the GP, the joint distribution has following form:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}f \\\\ f_* \\end{pmatrix}\n",
    "\\sim\n",
    "N\\Big(\n",
    "\\begin{pmatrix}\\mu \\\\ \\mu_* \\end{pmatrix},\n",
    "\\begin{pmatrix}\\mathbf{K} & \\mathbf{K_*}\\\\ \\mathbf{K_*^T} & \\mathbf{K_{**}} \\end{pmatrix}\n",
    "\\Big)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "By the standard fomular for conditioning Gaussian, the posterior has following form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{f_*| X_*,X,f}) & = N(\\mathbf{f_*|\\mu_*,\\Sigma_*}) \\\\\n",
    "\\mathbf{\\mu_*} & = \\mathbf{\\mu(X_*) + K_*^T K^{-1}(f-\\mu(X)) } \\\\\n",
    "\\mathbf{\\Sigma_*} & = \\mathbf{K_{**} - K_*^T K^{-1} K_*} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "A squared exponential is a common kernel, aka Gaussian kernel or RBF kernel. In 1d, it is:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\kappa(x,x') = \\sigma_f^2 exp(-\\frac{1}{2l^2}(x-x')^2)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Hyperparameters $l$ control the horizontal length scle and $\\sigma_f^2$ controls the vertical variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions using noisy observations\n",
    "Assume Gaussian noise, it's easy to get:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "cov[\\mathbf{y|X}] = \\mathbf{K} + \\sigma_y^2 \\mathbf{I}_N \\triangleq \\mathbf{K}_y\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Therefore, the joint distribution:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\\mathbf{y} \\\\ \\mathbf{f_*} \\end{pmatrix}\n",
    "\\sim\n",
    "N\\Big(\n",
    "\\mathbf{0},\n",
    "\\begin{pmatrix}\\mathbf{K}_y & \\mathbf{K_*}\\\\ \\mathbf{K_*^T} & \\mathbf{K_{**}} \\end{pmatrix}\n",
    "\\Big)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Posterior:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{f_*| X_*,X,y}) & = N(\\mathbf{f_*|\\mu_*,\\Sigma_*}) \\\\\n",
    "\\mathbf{\\mu_*} & = \\mathbf{ K_*^T K^{-1}y } \\\\\n",
    "\\mathbf{\\Sigma_*} & = \\mathbf{K_{**} - K_*^T K^{-1}_y K_*} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For single point estimate:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\bar{f}_* = \\mathbf{k_* K_y^{-1} y} = \\sum_{i=1}^N \\alpha_i \\kappa(\\mathbf{x}_i, \\mathbf{x}_*)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $\\alpha = \\mathbf{K}_y^{-1}\\mathbf{y}$. The SE kernel to multi-dimension:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\kappa_y(\\mathbf{x}_p,\\mathbf{x}_q) = \\sigma_f^2 exp(-\\frac{1}{2}(\\mathbf{x}_p - \\mathbf{x}_q)^T \\mathbf{M} (\\mathbf{x}_p - \\mathbf{x}_q)) + \\sigma_y^2 \\delta_{pq}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\mathbf{M}$ could be defined in many ways, the simplest one is isotropic matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel parameters estimation\n",
    "Exhaustive grid search is ok but slow. We could also use maximum marginal likelihood estimation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(\\mathbf{y|X}) = \\int p(\\mathbf{y|f,X}) p(\\mathbf{f|X}) d\\mathbf{f}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Everything is Gaussian, therefore we can derive the grad easily:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_j}log p(\\mathbf{y|X}) = \\frac{1}{2} tr\\Big((\\alpha \\alpha^T - \\mathbf{K}_y^{-1})\\frac{\\partial \\mathbf{K}_y}{\\partial \\theta_j}  \\Big)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial \\mathbf{K}_y}{\\partial \\theta_j}$ depends on the form of kernel. As the variance should be non-negative, we usually take $\\theta = log(\\sigma_y^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
